{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pwCaZX3mf9O",
        "outputId": "de960485-c966-4051-9c49-7fb761403c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/953.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m870.4/953.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-mwORTUlRLF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import namedtuple, deque\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoeUO3zWlRLJ"
      },
      "source": [
        "# Definition of a 3-layer neural net with tanh activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19xScikUlRLL"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
        "        super().__init__()\n",
        "        self.activation_function= nn.Tanh()\n",
        "\n",
        "        self.layer1 = nn.Linear( #<--- linear layer\n",
        "            n_inputs, #<----------------#input features\n",
        "            64,#<-----------------------#output features\n",
        "            bias=bias)#<----------------bias\n",
        "\n",
        "        self.layer2 = nn.Linear(\n",
        "            64,\n",
        "            32,\n",
        "            bias=bias)\n",
        "\n",
        "        self.layer3 = nn.Linear(\n",
        "                    32,\n",
        "                    n_outputs,\n",
        "                    bias=bias)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation_function( self.layer1(x) )\n",
        "        x = self.activation_function( self.layer2(x) )\n",
        "        y = self.layer3(x)\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_eLFU0blRLM"
      },
      "source": [
        "# Q network definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ39zCL-lRLM"
      },
      "outputs": [],
      "source": [
        "class Q_network(nn.Module):\n",
        "\n",
        "    def __init__(self, env,  learning_rate=1e-4):\n",
        "        super(Q_network, self).__init__()\n",
        "\n",
        "        n_outputs = env.action_space.n\n",
        "\n",
        "        # TODO\n",
        "        # self.network = Net( ?? , ??)\n",
        "        # NOTE: Network receives as input a state and must\n",
        "        # output a value for each action.\n",
        "        self.network = Net(1, 1)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(),\n",
        "                                          lr=learning_rate)\n",
        "\n",
        "    def greedy_action(self, state):\n",
        "        # TODO\n",
        "        # greedy action = ??\n",
        "        greedy_a = 0\n",
        "        return greedy_a\n",
        "\n",
        "    def get_qvals(self, state):\n",
        "        # TODO\n",
        "        #qval = ???\n",
        "        qval = 0\n",
        "        return qval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gharOAZ3lRLN"
      },
      "source": [
        "## Experience replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99zdRJ3ClRLN"
      },
      "outputs": [],
      "source": [
        "class Experience_replay_buffer:\n",
        "\n",
        "    def __init__(self, memory_size=50000, burn_in=10000):\n",
        "        self.memory_size = memory_size\n",
        "        self.burn_in = burn_in\n",
        "        self.Buffer = namedtuple('Buffer',\n",
        "                                 field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
        "        self.replay_memory = deque(maxlen=memory_size) #Double Ended Queue, efficient in appending and popping elements in it\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        samples = np.random.choice(len(self.replay_memory), batch_size,\n",
        "                                   replace=False)\n",
        "        # Use asterisk operator to unpack deque\n",
        "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
        "        return batch\n",
        "\n",
        "    def append(self, s_0, a, r, d, s_1):\n",
        "        self.replay_memory.append(\n",
        "            self.Buffer(s_0, a, r, d, s_1))\n",
        "\n",
        "    def burn_in_capacity(self):\n",
        "        return len(self.replay_memory) / self.burn_in\n",
        "\n",
        "    def capacity(self):\n",
        "        return len(self.replay_memory) / self.memory_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5TEby1LlRLN"
      },
      "source": [
        "# DDQN agent implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUXA2lWOlRLO"
      },
      "outputs": [],
      "source": [
        "def from_tuple_to_tensor(tuple_of_np):\n",
        "    tensor = torch.zeros((len(tuple_of_np), tuple_of_np[0].shape[0]))\n",
        "    for i, x in enumerate(tuple_of_np):\n",
        "        tensor[i] = torch.FloatTensor(x)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "class DDQN_agent:\n",
        "\n",
        "    def __init__(self, env, rew_thre, buffer, learning_rate=0.001, initial_epsilon=0.5, batch_size= 64):\n",
        "\n",
        "        self.env = env\n",
        "\n",
        "\n",
        "        self.network = Q_network(env, learning_rate).to(device)\n",
        "        # TODO\n",
        "        #self.target_network = ???\n",
        "        self.buffer = buffer\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.window = 50\n",
        "        self.reward_threshold = rew_thre\n",
        "        self.initialize()\n",
        "        self.step_count = 0\n",
        "        self.episode = 0\n",
        "\n",
        "\n",
        "    def take_step(self, mode='exploit'):\n",
        "        # choose action with epsilon greedy\n",
        "        if mode == 'explore':\n",
        "                action = self.env.action_space.sample()\n",
        "        else:\n",
        "                action = self.network.greedy_action(torch.FloatTensor(self.s_0).to(device))\n",
        "\n",
        "        #simulate action\n",
        "        s_1, r, terminated, truncated, _ = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        #put experience in the buffer\n",
        "        # TODO\n",
        "        #self.buffer ???\n",
        "\n",
        "        self.rewards += r\n",
        "\n",
        "        self.s_0 = s_1.copy()\n",
        "\n",
        "        self.step_count += 1\n",
        "        if done:\n",
        "            self.s_0, _ = self.env.reset()\n",
        "        return done\n",
        "\n",
        "    # Implement DQN training algorithm\n",
        "    def train(self, gamma=0.99, max_episodes=10000,\n",
        "              network_update_frequency=10,\n",
        "              network_sync_frequency=200):\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.loss_function = nn.MSELoss()\n",
        "        self.s_0, _ = self.env.reset()\n",
        "\n",
        "        # Populate replay buffer\n",
        "        while self.buffer.burn_in_capacity() < 1:\n",
        "            self.take_step(mode='explore')\n",
        "        ep = 0\n",
        "        training = True\n",
        "        self.populate = False\n",
        "        while training:\n",
        "            self.s_0, _ = self.env.reset()\n",
        "\n",
        "            self.rewards = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                if ((ep % 5) == 0):\n",
        "                    self.env.render()\n",
        "\n",
        "                p = np.random.random()\n",
        "                if p < self.epsilon:\n",
        "                    done = self.take_step(mode='explore')\n",
        "                    # print(\"explore\")\n",
        "                else:\n",
        "                    done = self.take_step(mode='exploit')\n",
        "                    # print(\"train\")\n",
        "                # Update network\n",
        "                if self.step_count % network_update_frequency == 0:\n",
        "                    self.update()\n",
        "                # Sync networks\n",
        "                if self.step_count % network_sync_frequency == 0:\n",
        "                    # TODO: synchronize Qnet and target_net\n",
        "                    #self.target_network ???\n",
        "                    self.sync_eps.append(ep)\n",
        "\n",
        "                if done:\n",
        "                    if self.epsilon >= 0.05:\n",
        "                        self.epsilon = self.epsilon * 0.7\n",
        "                    ep += 1\n",
        "                    if self.rewards > 2000:\n",
        "                        self.training_rewards.append(2000)\n",
        "                    elif self.rewards > 1000:\n",
        "                        self.training_rewards.append(1000)\n",
        "                    elif self.rewards > 500:\n",
        "                        self.training_rewards.append(500)\n",
        "                    else:\n",
        "                        self.training_rewards.append(self.rewards)\n",
        "                    if len(self.update_loss) == 0:\n",
        "                        self.training_loss.append(0)\n",
        "                    else:\n",
        "                        self.training_loss.append(np.mean(self.update_loss))\n",
        "                    self.update_loss = []\n",
        "                    mean_rewards = np.mean(self.training_rewards[-self.window:])\n",
        "                    mean_loss = np.mean(self.training_loss[-self.window:])\n",
        "                    self.mean_training_rewards.append(mean_rewards)\n",
        "                    print(\n",
        "                        \"\\rEpisode {:d} Mean Rewards {:.2f}  Episode reward = {:.2f}   mean loss = {:.2f}\\t\\t\".format(\n",
        "                            ep, mean_rewards, self.rewards, mean_loss), end=\"\")\n",
        "\n",
        "                    if ep >= max_episodes:\n",
        "                        training = False\n",
        "                        print('\\nEpisode limit reached.')\n",
        "                        break\n",
        "                    if mean_rewards >= self.reward_threshold:\n",
        "                        training = False\n",
        "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
        "                            ep))\n",
        "                        #break\n",
        "        # save models\n",
        "        self.save_models()\n",
        "        # plot\n",
        "        self.plot_training_rewards()\n",
        "\n",
        "    def save_models(self):\n",
        "        torch.save(self.network, \"Q_net\")\n",
        "\n",
        "    def load_models(self):\n",
        "        self.network = torch.load(\"Q_net\")\n",
        "        self.network.eval()\n",
        "\n",
        "    def plot_training_rewards(self):\n",
        "        plt.plot(self.mean_training_rewards)\n",
        "        plt.title('Mean training rewards')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.xlabel('Episods')\n",
        "        plt.show()\n",
        "        plt.savefig('mean_training_rewards.png')\n",
        "        plt.clf()\n",
        "\n",
        "    def calculate_loss(self, batch):\n",
        "        #extract info from batch\n",
        "        states, actions, rewards, dones, next_states = list(batch)\n",
        "\n",
        "        #transform in torch tensors\n",
        "        rewards = torch.FloatTensor(rewards).reshape(-1, 1).to(device)\n",
        "        actions = torch.LongTensor(np.array(actions)).reshape(-1, 1).to(device)\n",
        "        dones = torch.IntTensor(dones).reshape(-1, 1).to(device)\n",
        "        states = from_tuple_to_tensor(states).to(device)\n",
        "        next_states = from_tuple_to_tensor(next_states).to(device)\n",
        "\n",
        "        ###############\n",
        "        # DDQN Update #\n",
        "        ###############\n",
        "        # TODO\n",
        "        # Q(s,a) = ??\n",
        "        # Hint! You can use torch.gather to select the qvals\n",
        "        # corresponding to the actions in the batch.\n",
        "        #\n",
        "        #\n",
        "\n",
        "        # TODO\n",
        "        # target Q(s,a) = ??\n",
        "        #\n",
        "        #\n",
        "        #\n",
        "\n",
        "\n",
        "        # TODO\n",
        "        #loss = self.loss_function( Q(s,a) , target_Q(s,a))\n",
        "        loss = 0\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def update(self):\n",
        "        self.network.optimizer.zero_grad()\n",
        "        batch = self.buffer.sample_batch(batch_size=self.batch_size)\n",
        "        loss = self.calculate_loss(batch)\n",
        "\n",
        "        loss.backward()\n",
        "        self.network.optimizer.step()\n",
        "\n",
        "        self.update_loss.append(loss.item())\n",
        "\n",
        "    def initialize(self):\n",
        "        self.training_rewards = []\n",
        "        self.training_loss = []\n",
        "        self.update_loss = []\n",
        "        self.mean_training_rewards = []\n",
        "        self.sync_eps = []\n",
        "        self.rewards = 0\n",
        "        self.step_count = 0\n",
        "\n",
        "    def evaluate(self, eval_env):\n",
        "        done = False\n",
        "        s, _ = eval_env.reset()\n",
        "        rew = 0\n",
        "        while not done:\n",
        "            action = self.network.greedy_action(torch.FloatTensor(s).to(device))\n",
        "            s, r, terminated, truncated, _ = eval_env.step(action)\n",
        "            done = terminated or truncated\n",
        "            rew += r\n",
        "\n",
        "        print(\"Evaluation cumulative reward: \", rew)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAk-zkkblRLP"
      },
      "source": [
        "# Train and evaluate on cartpole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoZXxNoilRLQ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "rew_threshold = 200\n",
        "buffer = Experience_replay_buffer()\n",
        "agent = DDQN_agent(env, rew_threshold, buffer)\n",
        "agent.train()\n",
        "\n",
        "eval_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "agent.evaluate(eval_env)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
