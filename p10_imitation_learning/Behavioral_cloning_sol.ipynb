{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "691171dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable_baselines3 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (2.4.0)\n",
      "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from stable_baselines3) (0.29.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.20 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from stable_baselines3) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.13 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from stable_baselines3) (2.0.1)\n",
      "Requirement already satisfied: cloudpickle in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from stable_baselines3) (3.1.0)\n",
      "Requirement already satisfied: pandas in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from stable_baselines3) (2.0.3)\n",
      "Requirement already satisfied: matplotlib in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from stable_baselines3) (3.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (8.5.0)\n",
      "Requirement already satisfied: filelock in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (3.16.1)\n",
      "Requirement already satisfied: sympy in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13->stable_baselines3) (75.1.0)\n",
      "Requirement already satisfied: wheel in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13->stable_baselines3) (0.44.0)\n",
      "Requirement already satisfied: cmake in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.13->stable_baselines3) (3.30.4)\n",
      "Requirement already satisfied: lit in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.13->stable_baselines3) (18.1.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (10.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (6.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from pandas->stable_baselines3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from pandas->stable_baselines3) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gymnasium<1.1.0,>=0.29.1->stable_baselines3) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/federico/miniconda3/envs/RL/lib/python3.8/site-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b09a0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "#from .autonotebook import tqdm as notebook_tqdm\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d945059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define env\n",
    "env_id = \"CartPole-v1\"\n",
    "#env_id = \"Acrobot-v1\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3daca912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/federico/miniconda3/envs/RL/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 26.4       |\n",
      "|    ep_rew_mean          | 26.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 757        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00893118 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.686     |\n",
      "|    explained_variance   | -0.00266   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.36       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    value_loss           | 47.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 31.9        |\n",
      "|    ep_rew_mean          | 31.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 734         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008961838 |\n",
      "|    clip_fraction        | 0.0609      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0996      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 35.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 44.1         |\n",
      "|    ep_rew_mean          | 44.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 716          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077814506 |\n",
      "|    clip_fraction        | 0.0867       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.631       |\n",
      "|    explained_variance   | 0.222        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.4         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0181      |\n",
      "|    value_loss           | 50.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 58          |\n",
      "|    ep_rew_mean          | 58          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 695         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009643601 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 54.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 75           |\n",
      "|    ep_rew_mean          | 75           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 678          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076061785 |\n",
      "|    clip_fraction        | 0.07         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.588       |\n",
      "|    explained_variance   | 0.403        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.8         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0169      |\n",
      "|    value_loss           | 62           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 91.1         |\n",
      "|    ep_rew_mean          | 91.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 673          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067103915 |\n",
      "|    clip_fraction        | 0.0578       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.598       |\n",
      "|    explained_variance   | 0.655        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.77         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00946     |\n",
      "|    value_loss           | 44.4         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 109        |\n",
      "|    ep_rew_mean          | 109        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 660        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 24         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00627471 |\n",
      "|    clip_fraction        | 0.074      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.589     |\n",
      "|    explained_variance   | 0.712      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.71       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    value_loss           | 39.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 126         |\n",
      "|    ep_rew_mean          | 126         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 657         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009584396 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18          |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 44.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 143        |\n",
      "|    ep_rew_mean          | 143        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 660        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 31         |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00754006 |\n",
      "|    clip_fraction        | 0.0475     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.58      |\n",
      "|    explained_variance   | 0.772      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 26.5       |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.00506   |\n",
      "|    value_loss           | 44.9       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 157          |\n",
      "|    ep_rew_mean          | 157          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 662          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045210463 |\n",
      "|    clip_fraction        | 0.0568       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.572       |\n",
      "|    explained_variance   | 0.823        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.47         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00899     |\n",
      "|    value_loss           | 28.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 175          |\n",
      "|    ep_rew_mean          | 175          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057710856 |\n",
      "|    clip_fraction        | 0.0594       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.575       |\n",
      "|    explained_variance   | 0.947        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.26         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.01        |\n",
      "|    value_loss           | 12.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 193          |\n",
      "|    ep_rew_mean          | 193          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058643445 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.568       |\n",
      "|    explained_variance   | 0.682        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.214        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00582     |\n",
      "|    value_loss           | 4.55         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 212         |\n",
      "|    ep_rew_mean          | 212         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 654         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007699768 |\n",
      "|    clip_fraction        | 0.065       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.564      |\n",
      "|    explained_variance   | -0.07       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0569      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00254    |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 229          |\n",
      "|    ep_rew_mean          | 229          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015693761 |\n",
      "|    clip_fraction        | 0.00908      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.544       |\n",
      "|    explained_variance   | 0.000155     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.2         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 26.6         |\n",
      "------------------------------------------\n",
      "Mean reward expert agent= 500.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "#define expert agent\n",
    "ppo_expert = PPO('MlpPolicy', env_id, verbose=1)\n",
    "\n",
    "#train expert\n",
    "ppo_expert.learn(total_timesteps=3e4)\n",
    "\n",
    "#save expert\n",
    "ppo_expert.save(\"ppo_expert\")\n",
    "\n",
    "#evaluate expert\n",
    "mean_reward, std_reward = evaluate_policy(ppo_expert, Monitor(env), n_eval_episodes=10)\n",
    "print(f\"Mean reward expert agent= {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "480991d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 4)\n",
      "(40000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:19<00:00, 2045.37it/s]\n"
     ]
    }
   ],
   "source": [
    "##create expert dataset\n",
    "\n",
    "#empty dataset\n",
    "num_interactions = int(4e4)\n",
    "\n",
    "expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
    "expert_actions = np.empty((num_interactions,) + env.action_space.shape)\n",
    "\n",
    "print(expert_observations.shape)\n",
    "print(expert_actions.shape)\n",
    "\n",
    "#collect experience usign expert policy\n",
    "obs, _ = env.reset()\n",
    "for i in tqdm(range(num_interactions)):\n",
    "    action, _ = ppo_expert.predict(obs, deterministic=True)\n",
    "    expert_observations[i] = obs\n",
    "    expert_actions[i] = action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a75579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset\n",
    "np.savez_compressed(\n",
    "   \"expert_data\",\n",
    "   expert_actions=expert_actions,\n",
    "   expert_observations=expert_observations,\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce03945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##dataset class\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "\n",
    "class ExpertDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, expert_observations, expert_actions):\n",
    "        self.observations = expert_observations\n",
    "        self.actions = expert_actions\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.observations[index], self.actions[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11af5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "\n",
    "#split in 80% training and 20%test\n",
    "batch_size = 64\n",
    "train_prop = 0.8\n",
    "train_size = int(train_prop * len(expert_dataset))\n",
    "test_size = len(expert_dataset) - train_size\n",
    "train_expert_dataset, test_expert_dataset = random_split(expert_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = th.utils.data.DataLoader(  dataset=train_expert_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = th.utils.data.DataLoader(  dataset=test_expert_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc218b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_frames_as_gif(frames, path='./', filename='Behavioral_cloning.gif'):\n",
    "\n",
    "    #Mess with this to change frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c7a5b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda:  True\n"
     ]
    }
   ],
   "source": [
    "###### Define student agent\n",
    "no_cuda = False\n",
    "use_cuda = not no_cuda and th.cuda.is_available()\n",
    "print('use_cuda: ', use_cuda)\n",
    "   \n",
    "device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "class StudentAgent:\n",
    "    def __init__(self, env, train_loader, test_loader, learning_rate):\n",
    "        self.env = env\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        \n",
    "        n_inputs = env.observation_space.shape[0]\n",
    "        n_outputs = env.action_space.n\n",
    "        \n",
    "        # Defines a simple feedforward neural network with one hidden layer of size 16.\n",
    "        # Outputs probabilities for each action using a softmax.\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(n_inputs, 16), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(16, n_outputs),\n",
    "            nn.Softmax(dim=-1))\n",
    "        \n",
    "        print(\"policy net: \", self.policy)\n",
    "        \n",
    "        #TODO\n",
    "        self.loss_criterion = nn.CrossEntropyLoss()\n",
    "        #TODO\n",
    "        self.optimizer =  optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.num_eval_episodes = 10\n",
    "        \n",
    "    def train(self, num_epochs):\n",
    "        self.policy.train()\n",
    "        self.policy.to(device)\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                obs, expert_action = data.to(device), target.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                obs = obs.float()\n",
    "                #TODO\n",
    "                student_action = self.policy(obs)\n",
    "                expert_action = expert_action.long()\n",
    "                loss = self.loss_criterion(student_action, expert_action)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            #compute accuracy\n",
    "            train_acc = self.compute_accuracy(self.train_loader)\n",
    "            test_acc = self.compute_accuracy(self.test_loader)\n",
    "            policy_return = self.evaluate_policy(self.num_eval_episodes)\n",
    "            print(\"Epoch {}:\\ttrain accuracy: {}\\ttest accuracy: {}\\tpolicy return:{}\".format(epoch, train_acc, test_acc, policy_return))\n",
    "\n",
    "    def compute_accuracy(self, loader):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        self.policy.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                obs, expert_action = data.to(device), target.to(device)\n",
    "                obs = obs.float()\n",
    "                # TODO\n",
    "                student_action = self.policy_action(obs)\n",
    "            \n",
    "                total += student_action.size()[0]\n",
    "                correct += sum(student_action==expert_action).item()\n",
    "            \n",
    "        accuracy = 100. * correct/(float)(total)\n",
    "            \n",
    "        return accuracy\n",
    "            \n",
    "    \n",
    "    def policy_action(self, obs):\n",
    "        obs = obs.to(device)\n",
    "        #TODO\n",
    "        policy_act = self.policy(obs)\n",
    "        return th.argmax(policy_act, dim= 1)\n",
    "        \n",
    "    def evaluate_policy(self, num_episodes, render=False):\n",
    "        if render:\n",
    "            env = gym.make(self.env.spec.id, render_mode='rgb_array')\n",
    "        else:\n",
    "            env = self.env\n",
    "        rewards = []\n",
    "        \n",
    "        for ep in range(num_episodes):\n",
    "            done = False\n",
    "            tot_rew = 0\n",
    "            obs, _ = env.reset()\n",
    "            frames_gif=[]\n",
    "            while not done:\n",
    "                \n",
    "                obs = th.FloatTensor(obs).unsqueeze(0)\n",
    "                action = self.policy_action(obs)\n",
    "                obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "                done = terminated or truncated\n",
    "                if render==True and ep==num_episodes-1:\n",
    "                    frames_gif.append(env.render())\n",
    "                tot_rew += reward\n",
    "            rewards.append(tot_rew)\n",
    "        if render:\n",
    "            save_frames_as_gif(frames_gif)\n",
    "        return mean(rewards)\n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "614e3ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy net:  Sequential(\n",
      "  (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (3): Softmax(dim=-1)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\ttrain accuracy: 85.175\ttest accuracy: 85.175\tpolicy return:493.0\n",
      "Epoch 1:\ttrain accuracy: 94.5\ttest accuracy: 94.5\tpolicy return:500.0\n",
      "Epoch 2:\ttrain accuracy: 96.2375\ttest accuracy: 96.2375\tpolicy return:500.0\n",
      "Epoch 3:\ttrain accuracy: 96.475\ttest accuracy: 96.475\tpolicy return:500.0\n",
      "Epoch 4:\ttrain accuracy: 96.9875\ttest accuracy: 96.9875\tpolicy return:500.0\n",
      "Epoch 5:\ttrain accuracy: 97.2\ttest accuracy: 97.2\tpolicy return:500.0\n",
      "Epoch 6:\ttrain accuracy: 97.2\ttest accuracy: 97.2\tpolicy return:500.0\n",
      "Epoch 7:\ttrain accuracy: 95.8\ttest accuracy: 95.8\tpolicy return:500.0\n",
      "Epoch 8:\ttrain accuracy: 97.6875\ttest accuracy: 97.6875\tpolicy return:500.0\n",
      "Epoch 9:\ttrain accuracy: 97.7375\ttest accuracy: 97.7375\tpolicy return:500.0\n",
      "Epoch 10:\ttrain accuracy: 97.8875\ttest accuracy: 97.8875\tpolicy return:500.0\n",
      "Epoch 11:\ttrain accuracy: 97.9875\ttest accuracy: 97.9875\tpolicy return:500.0\n",
      "Epoch 12:\ttrain accuracy: 98.5875\ttest accuracy: 98.5875\tpolicy return:500.0\n",
      "Epoch 13:\ttrain accuracy: 98.7\ttest accuracy: 98.7\tpolicy return:500.0\n",
      "Epoch 14:\ttrain accuracy: 97.4875\ttest accuracy: 97.4875\tpolicy return:500.0\n",
      "Epoch 15:\ttrain accuracy: 99.0125\ttest accuracy: 99.0125\tpolicy return:500.0\n",
      "Epoch 16:\ttrain accuracy: 98.7125\ttest accuracy: 98.7125\tpolicy return:500.0\n",
      "Epoch 17:\ttrain accuracy: 98.5875\ttest accuracy: 98.5875\tpolicy return:500.0\n",
      "Epoch 18:\ttrain accuracy: 99.1875\ttest accuracy: 99.1875\tpolicy return:500.0\n",
      "Epoch 19:\ttrain accuracy: 99.3125\ttest accuracy: 99.3125\tpolicy return:500.0\n",
      "Epoch 20:\ttrain accuracy: 99.1375\ttest accuracy: 99.1375\tpolicy return:500.0\n",
      "Epoch 21:\ttrain accuracy: 96.275\ttest accuracy: 96.275\tpolicy return:500.0\n",
      "Epoch 22:\ttrain accuracy: 98.6625\ttest accuracy: 98.6625\tpolicy return:500.0\n",
      "Epoch 23:\ttrain accuracy: 99.225\ttest accuracy: 99.225\tpolicy return:500.0\n",
      "Epoch 24:\ttrain accuracy: 98.8375\ttest accuracy: 98.8375\tpolicy return:500.0\n",
      "Epoch 25:\ttrain accuracy: 98.525\ttest accuracy: 98.525\tpolicy return:500.0\n",
      "Epoch 26:\ttrain accuracy: 98.725\ttest accuracy: 98.725\tpolicy return:500.0\n",
      "Epoch 27:\ttrain accuracy: 98.25\ttest accuracy: 98.25\tpolicy return:500.0\n",
      "Epoch 28:\ttrain accuracy: 99.15\ttest accuracy: 99.15\tpolicy return:500.0\n",
      "Epoch 29:\ttrain accuracy: 99.4375\ttest accuracy: 99.4375\tpolicy return:500.0\n",
      "Epoch 30:\ttrain accuracy: 99.5\ttest accuracy: 99.5\tpolicy return:500.0\n",
      "Epoch 31:\ttrain accuracy: 99.5125\ttest accuracy: 99.5125\tpolicy return:500.0\n",
      "Epoch 32:\ttrain accuracy: 99.3375\ttest accuracy: 99.3375\tpolicy return:500.0\n",
      "Epoch 33:\ttrain accuracy: 98.9\ttest accuracy: 98.9\tpolicy return:500.0\n",
      "Epoch 34:\ttrain accuracy: 99.1875\ttest accuracy: 99.1875\tpolicy return:500.0\n",
      "Epoch 35:\ttrain accuracy: 98.3375\ttest accuracy: 98.3375\tpolicy return:500.0\n",
      "Epoch 36:\ttrain accuracy: 98.7625\ttest accuracy: 98.7625\tpolicy return:500.0\n",
      "Epoch 37:\ttrain accuracy: 99.45\ttest accuracy: 99.45\tpolicy return:500.0\n",
      "Epoch 38:\ttrain accuracy: 99.5375\ttest accuracy: 99.5375\tpolicy return:500.0\n",
      "Epoch 39:\ttrain accuracy: 98.2875\ttest accuracy: 98.2875\tpolicy return:500.0\n",
      "Epoch 40:\ttrain accuracy: 99.5125\ttest accuracy: 99.5125\tpolicy return:500.0\n",
      "Epoch 41:\ttrain accuracy: 99.3625\ttest accuracy: 99.3625\tpolicy return:500.0\n",
      "Epoch 42:\ttrain accuracy: 99.425\ttest accuracy: 99.425\tpolicy return:500.0\n",
      "Epoch 43:\ttrain accuracy: 99.475\ttest accuracy: 99.475\tpolicy return:500.0\n",
      "Epoch 44:\ttrain accuracy: 98.7375\ttest accuracy: 98.7375\tpolicy return:500.0\n",
      "Epoch 45:\ttrain accuracy: 99.4125\ttest accuracy: 99.4125\tpolicy return:500.0\n",
      "Epoch 46:\ttrain accuracy: 98.8125\ttest accuracy: 98.8125\tpolicy return:500.0\n",
      "Epoch 47:\ttrain accuracy: 98.0125\ttest accuracy: 98.0125\tpolicy return:500.0\n",
      "Epoch 48:\ttrain accuracy: 99.75\ttest accuracy: 99.75\tpolicy return:500.0\n",
      "Epoch 49:\ttrain accuracy: 98.0\ttest accuracy: 98.0\tpolicy return:500.0\n"
     ]
    }
   ],
   "source": [
    "student = StudentAgent(env, train_loader, test_loader, 0.01)\n",
    "student.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00442cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "student.evaluate_policy(1, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
