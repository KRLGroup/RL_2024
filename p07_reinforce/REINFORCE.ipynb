{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "VjX_2mvrN7c1",
    "outputId": "739f2177-ada7-4f2b-8166-0151790723ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_nqTKTwODqj",
    "outputId": "b7d99f31-d263-4fca-f4be-5757ca29b61e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.1.1.post1)\n",
      "Requirement already satisfied: gymnasium in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium[box2d]) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium[box2d]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium[box2d]) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium[box2d]) (2.5.1)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium[box2d]) (4.1.1.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Aq405erfpGKv"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raB0pq_vuaak",
    "outputId": "58b2e7f3-6224-40ee-ced3-ef6bd1a42f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "x5Y4Y3eXugTc"
   },
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "\n",
    "# Create the env\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "\n",
    "# Get the state space and action space\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NpSZGAuKulI6"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, n_actions, hidden_size, img_size=(64,64), device=torch.device('cpu')):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "    def act(self, state, exploration=True):\n",
    "\n",
    "        # Get Action Probabilities\n",
    "        probs = self.forward(state).cpu()\n",
    "\n",
    "        # Return Action and LogProb\n",
    "        action = probs.argmax(dim=-1)\n",
    "        log_prob = None\n",
    "        if exploration:\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample()\n",
    "            log_prob = m.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def to(self, device):\n",
    "        ret = super().to(device)\n",
    "        ret.device = device\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sYk1se-R3vmh"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, n_eval_episodes, policy):\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state, _ = env.reset() # state reset\n",
    "\n",
    "        done = trunc = False\n",
    "\n",
    "        # stats\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        while not done and not trunc:\n",
    "            # perform action\n",
    "            action, _ = policy.act(torch.tensor(state).to(device), exploration=False)\n",
    "\n",
    "            # As in practical 4, we do not consider \"truncated\" (i.e., reaching the max number of steps)\n",
    "            # as a termination condition\n",
    "            state, reward, done, trunc, _ = env.step(action)\n",
    "\n",
    "            # stats\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "        # stats\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "\n",
    "    # stats\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uzb4bInRxMsx"
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes=50, gamma=0.99, print_every=10):\n",
    "    # stats\n",
    "    scores_deque = deque(maxlen=100)\n",
    "\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = [] # stores log probs during episode\n",
    "        rewards = [] # stores rewards during episode\n",
    "\n",
    "        # init episode\n",
    "        state, _ = env.reset()\n",
    "        done = trunc = False\n",
    "\n",
    "        while not done and not trunc:\n",
    "            action, log_prob = policy.act(torch.tensor(state).to(device))\n",
    "\n",
    "            saved_log_probs.append(log_prob)\n",
    "\n",
    "            state, reward, done, trunc, _ = env.step(action)\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "        scores_deque.append(sum(rewards))\n",
    "\n",
    "\n",
    "        rewards = np.array(rewards)\n",
    "        discounts = np.power(gamma, np.arange(len(rewards)))\n",
    "\n",
    "        policy_loss = 0\n",
    "        for t in range(len(rewards)):\n",
    "            # Compute Gt using discounts and rewards\n",
    "            G = ... # TODO\n",
    "            # Compute the policy loss term for t using saved_log_probs, G and gamma\n",
    "            policy_loss += ... # TODO\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print(f'''Episode {i_episode}\n",
    "                    \\tAverage Score: {np.mean(scores_deque)}\n",
    "                    \\tLast Score: {rewards.sum()}\n",
    "                    \\tEval Score: {evaluate_agent(eval_env,5,policy)}''')\n",
    "            torch.save(policy, 'model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GwVv1ETxN7c6"
   },
   "outputs": [],
   "source": [
    "policy = Policy(env.observation_space.shape[0], n_actions, 32).to(device)\n",
    "policy = policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bQYqOdbiy0ez"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "esS1pRU6D9CS",
    "outputId": "022bf43b-7292-4757-9183-6cf61226f22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\n",
      "                    \tAverage Score: 18.8\n",
      "                    \tLast Score: 45.0\n",
      "                    \tEval Score: (9.8, 0.9797958971132712)\n",
      "Episode 20\n",
      "                    \tAverage Score: 23.9\n",
      "                    \tLast Score: 26.0\n",
      "                    \tEval Score: (9.6, 0.7999999999999999)\n",
      "Episode 30\n",
      "                    \tAverage Score: 23.166666666666668\n",
      "                    \tLast Score: 15.0\n",
      "                    \tEval Score: (9.8, 0.7483314773547882)\n",
      "Episode 40\n",
      "                    \tAverage Score: 21.875\n",
      "                    \tLast Score: 21.0\n",
      "                    \tEval Score: (11.4, 1.3564659966250536)\n",
      "Episode 50\n",
      "                    \tAverage Score: 21.2\n",
      "                    \tLast Score: 17.0\n",
      "                    \tEval Score: (12.4, 1.4966629547095764)\n",
      "Episode 60\n",
      "                    \tAverage Score: 22.183333333333334\n",
      "                    \tLast Score: 32.0\n",
      "                    \tEval Score: (19.6, 2.727636339397171)\n",
      "Episode 70\n",
      "                    \tAverage Score: 24.271428571428572\n",
      "                    \tLast Score: 18.0\n",
      "                    \tEval Score: (19.0, 3.687817782917155)\n",
      "Episode 80\n",
      "                    \tAverage Score: 24.475\n",
      "                    \tLast Score: 19.0\n",
      "                    \tEval Score: (23.4, 3.8781438859330635)\n",
      "Episode 90\n",
      "                    \tAverage Score: 24.944444444444443\n",
      "                    \tLast Score: 14.0\n",
      "                    \tEval Score: (23.4, 2.939387691339814)\n",
      "Episode 100\n",
      "                    \tAverage Score: 25.31\n",
      "                    \tLast Score: 14.0\n",
      "                    \tEval Score: (47.2, 20.232646885664764)\n",
      "Episode 110\n",
      "                    \tAverage Score: 25.81\n",
      "                    \tLast Score: 13.0\n",
      "                    \tEval Score: (58.0, 18.64403389827427)\n",
      "Episode 120\n",
      "                    \tAverage Score: 25.74\n",
      "                    \tLast Score: 43.0\n",
      "                    \tEval Score: (41.0, 7.0710678118654755)\n",
      "Episode 130\n",
      "                    \tAverage Score: 26.0\n",
      "                    \tLast Score: 15.0\n",
      "                    \tEval Score: (63.6, 25.152335875619983)\n",
      "Episode 140\n",
      "                    \tAverage Score: 26.83\n",
      "                    \tLast Score: 14.0\n",
      "                    \tEval Score: (158.8, 88.16439190512233)\n",
      "Episode 150\n",
      "                    \tAverage Score: 27.79\n",
      "                    \tLast Score: 19.0\n",
      "                    \tEval Score: (179.6, 83.38728919925386)\n",
      "Episode 160\n",
      "                    \tAverage Score: 27.5\n",
      "                    \tLast Score: 22.0\n",
      "                    \tEval Score: (144.6, 36.98972830394946)\n",
      "Episode 170\n",
      "                    \tAverage Score: 26.17\n",
      "                    \tLast Score: 37.0\n",
      "                    \tEval Score: (68.8, 18.454267799075637)\n",
      "Episode 180\n",
      "                    \tAverage Score: 26.5\n",
      "                    \tLast Score: 13.0\n",
      "                    \tEval Score: (70.2, 31.795597179483828)\n",
      "Episode 190\n",
      "                    \tAverage Score: 27.2\n",
      "                    \tLast Score: 44.0\n",
      "                    \tEval Score: (75.0, 22.44994432064365)\n",
      "Episode 200\n",
      "                    \tAverage Score: 27.94\n",
      "                    \tLast Score: 31.0\n",
      "                    \tEval Score: (66.2, 17.428711943227473)\n",
      "Episode 210\n",
      "                    \tAverage Score: 30.07\n",
      "                    \tLast Score: 61.0\n",
      "                    \tEval Score: (61.2, 12.416118556135006)\n",
      "Episode 220\n",
      "                    \tAverage Score: 30.72\n",
      "                    \tLast Score: 46.0\n",
      "                    \tEval Score: (70.8, 28.826376810136928)\n",
      "Episode 230\n",
      "                    \tAverage Score: 31.89\n",
      "                    \tLast Score: 75.0\n",
      "                    \tEval Score: (57.4, 13.32066064427737)\n",
      "Episode 240\n",
      "                    \tAverage Score: 32.09\n",
      "                    \tLast Score: 24.0\n",
      "                    \tEval Score: (59.8, 15.197368193210297)\n",
      "Episode 250\n",
      "                    \tAverage Score: 33.6\n",
      "                    \tLast Score: 49.0\n",
      "                    \tEval Score: (46.4, 11.359577456930342)\n",
      "Episode 260\n",
      "                    \tAverage Score: 35.47\n",
      "                    \tLast Score: 23.0\n",
      "                    \tEval Score: (65.8, 23.91150350772615)\n",
      "Episode 270\n",
      "                    \tAverage Score: 36.34\n",
      "                    \tLast Score: 22.0\n",
      "                    \tEval Score: (100.0, 67.62839640269463)\n",
      "Episode 280\n",
      "                    \tAverage Score: 38.35\n",
      "                    \tLast Score: 26.0\n",
      "                    \tEval Score: (48.2, 20.063897926375123)\n",
      "Episode 290\n",
      "                    \tAverage Score: 37.93\n",
      "                    \tLast Score: 54.0\n",
      "                    \tEval Score: (37.2, 5.2306787322488075)\n",
      "Episode 300\n",
      "                    \tAverage Score: 39.22\n",
      "                    \tLast Score: 127.0\n",
      "                    \tEval Score: (47.0, 15.006665185843255)\n",
      "Episode 310\n",
      "                    \tAverage Score: 39.54\n",
      "                    \tLast Score: 25.0\n",
      "                    \tEval Score: (74.8, 50.244999751219034)\n",
      "Episode 320\n",
      "                    \tAverage Score: 41.72\n",
      "                    \tLast Score: 59.0\n",
      "                    \tEval Score: (50.8, 18.258148865643527)\n",
      "Episode 330\n",
      "                    \tAverage Score: 44.5\n",
      "                    \tLast Score: 68.0\n",
      "                    \tEval Score: (118.6, 62.0728604141939)\n",
      "Episode 340\n",
      "                    \tAverage Score: 48.41\n",
      "                    \tLast Score: 66.0\n",
      "                    \tEval Score: (243.0, 156.08074833239365)\n",
      "Episode 350\n",
      "                    \tAverage Score: 50.67\n",
      "                    \tLast Score: 23.0\n",
      "                    \tEval Score: (120.8, 66.70652142032291)\n",
      "Episode 360\n",
      "                    \tAverage Score: 52.77\n",
      "                    \tLast Score: 110.0\n",
      "                    \tEval Score: (166.8, 106.40187968264469)\n",
      "Episode 370\n",
      "                    \tAverage Score: 55.31\n",
      "                    \tLast Score: 11.0\n",
      "                    \tEval Score: (93.8, 32.014996486021985)\n",
      "Episode 380\n",
      "                    \tAverage Score: 56.34\n",
      "                    \tLast Score: 21.0\n",
      "                    \tEval Score: (109.0, 42.881231325604446)\n",
      "Episode 390\n",
      "                    \tAverage Score: 58.2\n",
      "                    \tLast Score: 22.0\n",
      "                    \tEval Score: (97.0, 21.98181066245454)\n",
      "Episode 400\n",
      "                    \tAverage Score: 58.25\n",
      "                    \tLast Score: 31.0\n",
      "                    \tEval Score: (176.6, 163.64424829489118)\n",
      "Episode 410\n",
      "                    \tAverage Score: 61.66\n",
      "                    \tLast Score: 218.0\n",
      "                    \tEval Score: (173.0, 127.35462300207244)\n",
      "Episode 420\n",
      "                    \tAverage Score: 63.58\n",
      "                    \tLast Score: 79.0\n",
      "                    \tEval Score: (163.0, 110.4192012287718)\n",
      "Episode 430\n",
      "                    \tAverage Score: 65.68\n",
      "                    \tLast Score: 64.0\n",
      "                    \tEval Score: (134.4, 27.353975945006606)\n",
      "Episode 440\n",
      "                    \tAverage Score: 64.88\n",
      "                    \tLast Score: 64.0\n",
      "                    \tEval Score: (285.6, 94.88856622375533)\n",
      "Episode 450\n",
      "                    \tAverage Score: 63.69\n",
      "                    \tLast Score: 40.0\n",
      "                    \tEval Score: (237.0, 100.1359076455594)\n",
      "Episode 460\n",
      "                    \tAverage Score: 65.06\n",
      "                    \tLast Score: 92.0\n",
      "                    \tEval Score: (231.6, 142.2457029227948)\n",
      "Episode 470\n",
      "                    \tAverage Score: 67.02\n",
      "                    \tLast Score: 22.0\n",
      "                    \tEval Score: (274.4, 138.42196357514945)\n",
      "Episode 480\n",
      "                    \tAverage Score: 69.46\n",
      "                    \tLast Score: 80.0\n",
      "                    \tEval Score: (158.6, 50.835420722169694)\n",
      "Episode 490\n",
      "                    \tAverage Score: 73.24\n",
      "                    \tLast Score: 134.0\n",
      "                    \tEval Score: (126.0, 13.29661611087573)\n",
      "Episode 500\n",
      "                    \tAverage Score: 76.81\n",
      "                    \tLast Score: 119.0\n",
      "                    \tEval Score: (211.6, 66.43673682534386)\n",
      "Episode 510\n",
      "                    \tAverage Score: 79.23\n",
      "                    \tLast Score: 151.0\n",
      "                    \tEval Score: (301.4, 162.98662521814484)\n",
      "Episode 520\n",
      "                    \tAverage Score: 81.43\n",
      "                    \tLast Score: 76.0\n",
      "                    \tEval Score: (457.8, 84.39999999999999)\n",
      "Episode 530\n",
      "                    \tAverage Score: 84.04\n",
      "                    \tLast Score: 51.0\n",
      "                    \tEval Score: (500.0, 0.0)\n",
      "Episode 540\n",
      "                    \tAverage Score: 89.77\n",
      "                    \tLast Score: 211.0\n",
      "                    \tEval Score: (499.8, 0.4)\n",
      "Episode 550\n",
      "                    \tAverage Score: 95.68\n",
      "                    \tLast Score: 182.0\n",
      "                    \tEval Score: (499.8, 0.4)\n",
      "Episode 560\n",
      "                    \tAverage Score: 101.3\n",
      "                    \tLast Score: 161.0\n",
      "                    \tEval Score: (448.4, 88.96201436568307)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreinforce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_training_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mreinforce\u001b[0;34m(policy, optimizer, n_training_episodes, gamma, print_every)\u001b[0m\n\u001b[1;32m     11\u001b[0m done \u001b[38;5;241m=\u001b[39m trunc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trunc:\n\u001b[0;32m---> 14\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mact(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m     saved_log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n\u001b[1;32m     18\u001b[0m     state, reward, done, trunc, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reinforce(policy, optimizer, n_training_episodes=1000, gamma=0.99, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85P7ChOqN7c8",
    "outputId": "507d3997-1d9a-44cd-a59d-755b5373487d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqv8LscQrgn"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLb0dedAQtx4",
    "outputId": "cfbb48a5-c355-4cc0-dceb-90812bfc65e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=5, bias=True)\n",
       "  (gs): Grayscale(num_output_channels=1)\n",
       "  (rs): Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = torch.load('model.pt', map_location=device)\n",
    "policy = policy.to(device)\n",
    "policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_frames_as_gif(frames, path='./', filename='REINFORCE.gif'):\n",
    "\n",
    "    #Mess with this to change frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "cq28HU1UTEBo"
   },
   "outputs": [],
   "source": [
    "def play_agent(env, policy):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    frames_gif=[]\n",
    "    for i in range(60):\n",
    "        state,_,_,_,_ = env.step(0)\n",
    "    step = 0\n",
    "    done = False\n",
    "    negative_reward_patience = MAX_PATIENCE\n",
    "    states = deque(maxlen=4)\n",
    "    for i in range(policy.n_frames):\n",
    "        states.append(state)\n",
    "    while not done:\n",
    "        \n",
    "        action, _ = policy.act(states, exploration=False)\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        states.append(new_state)\n",
    "        if reward >=0:\n",
    "            negative_reward_patience = MAX_PATIENCE\n",
    "        else:\n",
    "            negative_reward_patience -= 1\n",
    "            if negative_reward_patience == 0:\n",
    "                done = True\n",
    "        if done:\n",
    "            reward = -100\n",
    "        total_reward += reward\n",
    "        frames_gif.append(env.render())\n",
    "        if done:\n",
    "            break\n",
    "        state = new_state\n",
    "    save_frames_as_gif(frames_gif)\n",
    "    print(\"Total Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRWzLGZVTMiV",
    "outputId": "823268d1-5b75-413b-aab3-8341e8239700"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: -109.89999999999998\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "play_agent(eval_env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tutor-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
