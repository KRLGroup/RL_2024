{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VjX_2mvrN7c1",
        "outputId": "739f2177-ada7-4f2b-8166-0151790723ae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_nqTKTwODqj",
        "outputId": "b7d99f31-d263-4fca-f4be-5757ca29b61e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.1.1)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Aq405erfpGKv"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import pickle\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raB0pq_vuaak",
        "outputId": "58b2e7f3-6224-40ee-ced3-ef6bd1a42f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x5Y4Y3eXugTc"
      },
      "outputs": [],
      "source": [
        "env_id = \"CarRacing-v2\"\n",
        "\n",
        "# Create the env\n",
        "env = gym.make(env_id, continuous=False, domain_randomize=False)\n",
        "\n",
        "# Create the evaluation env\n",
        "eval_env = gym.make(env_id, continuous=False, domain_randomize=False, render_mode=\"human\")\n",
        "\n",
        "# Get the state space and action space\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "n_frames = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NpSZGAuKulI6"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, n_frames, n_actions, hidden_size, img_size=(64,64), device=torch.device('cpu')):\n",
        "        super(Policy, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_frames = n_frames\n",
        "        self.conv1 = nn.Conv2d(n_frames, hidden_size, 7)\n",
        "        self.conv2 = nn.Conv2d(hidden_size, hidden_size, 5)\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
        "        self.gs = transforms.Grayscale()\n",
        "        self.rs = transforms.Resize(img_size)\n",
        "        self.device = device\n",
        "\n",
        "    def preproc_state(self, state):\n",
        "        # State Preprocessing\n",
        "        state = state[:83,:].transpose(2,0,1) #Torch wants images in format (channels, height, width)\n",
        "        state = torch.from_numpy(state)\n",
        "        state = self.gs(state) # grayscale\n",
        "        state = self.rs(state) # resize\n",
        "        return state/255 # normalize\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutions\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        # Global Max Pooling\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.reshape(batch_size, self.hidden_size, -1).max(axis=2).values\n",
        "\n",
        "        # Layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "    def act(self, states, exploration=True):\n",
        "        # Stack 4 states\n",
        "        state = torch.vstack([self.preproc_state(state) for state in states]).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # Get Action Probabilities\n",
        "        probs = self.forward(state).cpu()\n",
        "\n",
        "        # Return Action and LogProb\n",
        "        action = probs.argmax(-1)\n",
        "        log_prob = None\n",
        "        if exploration:\n",
        "            m = Categorical(probs)\n",
        "            action = m.sample()\n",
        "            log_prob = m.log_prob(action)\n",
        "        return action.item(), log_prob\n",
        "\n",
        "    def to(self, device):\n",
        "        ret = super().to(device)\n",
        "        ret.device = device\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5kAeq1Rl1Hyj"
      },
      "outputs": [],
      "source": [
        "MAX_PATIENCE = 100 # Maximum consecutive steps with negative reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sYk1se-R3vmh"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, n_eval_episodes, policy):\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(n_eval_episodes):\n",
        "        state = env.reset() # state reset\n",
        "\n",
        "        # perform noop for 60 steps (noisy start)\n",
        "        for i in range(60):\n",
        "            state,_,_,_,_ = env.step(0)\n",
        "\n",
        "\n",
        "        done = False\n",
        "\n",
        "        # stats\n",
        "        total_rewards_ep = 0\n",
        "        negative_reward_patience = MAX_PATIENCE\n",
        "\n",
        "        # state\n",
        "        states = deque(maxlen=4)\n",
        "        for i in range(n_frames):\n",
        "            states.append(state)\n",
        "\n",
        "        while not done:\n",
        "            # perform action\n",
        "            action, _ = policy.act(states, exploration=False)\n",
        "\n",
        "            # As in practical 4, we do not consider \"truncated\" (i.e., reaching the max number of steps)\n",
        "            # as a termination condition\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "            states.append(state)\n",
        "\n",
        "            # handle patience\n",
        "            if reward >=0:\n",
        "                negative_reward_patience = MAX_PATIENCE\n",
        "            else:\n",
        "                negative_reward_patience -= 1\n",
        "                if negative_reward_patience == 0:\n",
        "                    done = True\n",
        "            if done: reward = -100\n",
        "\n",
        "            # stats\n",
        "            total_rewards_ep += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # stats\n",
        "        episode_rewards.append(total_rewards_ep)\n",
        "\n",
        "    # stats\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "\n",
        "    return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Uzb4bInRxMsx"
      },
      "outputs": [],
      "source": [
        "def reinforce(policy, optimizer, n_training_episodes=1000, gamma=0.99, print_every=5):\n",
        "    # stats\n",
        "    scores_deque = deque(maxlen=100)\n",
        "\n",
        "    for i_episode in range(1, n_training_episodes+1):\n",
        "        saved_log_probs = [] # stores log probs during episode\n",
        "        rewards = [] # stores rewards during episode\n",
        "\n",
        "        # init episode\n",
        "        state = env.reset()\n",
        "        for i in range(60):\n",
        "            state,_,_,_,_ = env.step(0)\n",
        "        done = False\n",
        "\n",
        "        negative_reward_patience = MAX_PATIENCE\n",
        "        states = deque(maxlen=4)\n",
        "        for i in range(n_frames):\n",
        "            states.append(state)\n",
        "\n",
        "\n",
        "        while not done:\n",
        "            action, log_prob = policy.act(states)\n",
        "\n",
        "            saved_log_probs.append(log_prob)\n",
        "\n",
        "            # As in practical 4, we do not consider \"truncated\" (i.e., reaching the max number of steps)\n",
        "            # as a termination condition\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            states.append(state)\n",
        "\n",
        "            if reward >=0:\n",
        "                negative_reward_patience = MAX_PATIENCE\n",
        "            else:\n",
        "                negative_reward_patience -= 1\n",
        "                if negative_reward_patience == 0:\n",
        "                    done = True\n",
        "            if done: reward = -100\n",
        "\n",
        "            rewards.append(reward)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        scores_deque.append(sum(rewards))\n",
        "\n",
        "\n",
        "        rewards = np.array(rewards)\n",
        "        discounts = np.power(gamma, np.arange(len(rewards)))\n",
        "\n",
        "        policy_loss = 0\n",
        "        for t in range(len(rewards)):\n",
        "            G = (discounts[:len(rewards)-t]*rewards[t:]).sum()\n",
        "            policy_loss += -(gamma**t)*G*saved_log_probs[t]\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i_episode % print_every == 0:\n",
        "            print(f'''Episode {i_episode}\n",
        "                    \\tAverage Score: {np.mean(scores_deque)}\n",
        "                    \\tLast Score: {rewards.sum()}\n",
        "                    \\tEval Score: {evaluate_agent(eval_env,5,policy)}''')\n",
        "            torch.save(policy, 'model.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GwVv1ETxN7c6"
      },
      "outputs": [],
      "source": [
        "policy = Policy(n_frames, n_actions, 32).to(device)\n",
        "policy = policy.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bQYqOdbiy0ez"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(policy.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "esS1pRU6D9CS",
        "outputId": "022bf43b-7292-4757-9183-6cf61226f22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 5\n",
            "                    \tAverage Score: -119.30955029130754\n",
            "                    \tLast Score: -115.40780141843967\n",
            "                    \tEval Score: (-109.89999999999998, 0.0)\n",
            "Episode 10\n",
            "                    \tAverage Score: -121.08945043890724\n",
            "                    \tLast Score: -133.64031007751964\n",
            "                    \tEval Score: (-109.89999999999998, 0.0)\n",
            "Episode 15\n",
            "                    \tAverage Score: -125.08511280852673\n",
            "                    \tLast Score: -133.72835820895608\n",
            "                    \tEval Score: (-109.89999999999998, 0.0)\n",
            "Episode 20\n",
            "                    \tAverage Score: -127.12073724431522\n",
            "                    \tLast Score: -127.04142394822006\n",
            "                    \tEval Score: (-57.263549090964226, 3.5202065437177463)\n",
            "Episode 25\n",
            "                    \tAverage Score: -120.02256333716119\n",
            "                    \tLast Score: -96.13846153846141\n",
            "                    \tEval Score: (-41.068611381081716, 29.814688222806556)\n",
            "Episode 30\n",
            "                    \tAverage Score: -111.02593578029159\n",
            "                    \tLast Score: -71.87931034482786\n",
            "                    \tEval Score: (-56.01776532024843, 5.897219177769267)\n",
            "Episode 35\n",
            "                    \tAverage Score: -104.6603284361888\n",
            "                    \tLast Score: -67.87707641196036\n",
            "                    \tEval Score: (-44.442255937592435, 24.405762186559564)\n",
            "Episode 40\n",
            "                    \tAverage Score: -97.20621461430977\n",
            "                    \tLast Score: -59.06330935251832\n",
            "                    \tEval Score: (-42.64352226640213, 35.46169857597335)\n",
            "Episode 45\n",
            "                    \tAverage Score: -94.7190804404573\n",
            "                    \tLast Score: -76.775159235669\n",
            "                    \tEval Score: (-48.88101493472188, 17.29199342796758)\n",
            "Episode 50\n",
            "                    \tAverage Score: -92.33231561388708\n",
            "                    \tLast Score: -70.83448275862094\n",
            "                    \tEval Score: (-41.80414690122771, 27.40355704825534)\n",
            "Episode 55\n",
            "                    \tAverage Score: -90.87002366083037\n",
            "                    \tLast Score: -87.19249146757694\n",
            "                    \tEval Score: (-109.89999999999998, 0.0)\n",
            "Episode 60\n",
            "                    \tAverage Score: -89.41537909397721\n",
            "                    \tLast Score: -62.507299270073254\n",
            "                    \tEval Score: (-109.89999999999998, 0.0)\n",
            "Episode 65\n",
            "                    \tAverage Score: -88.0134345537451\n",
            "                    \tLast Score: -35.34374999999886\n",
            "                    \tEval Score: (-109.89999999999998, 0.0)\n",
            "Episode 70\n",
            "                    \tAverage Score: -87.79472869425133\n",
            "                    \tLast Score: -81.09859154929612\n",
            "                    \tEval Score: (-109.89999999999998, 0.0)\n",
            "Episode 75\n",
            "                    \tAverage Score: -87.22402002926806\n",
            "                    \tLast Score: -84.00684931506866\n",
            "                    \tEval Score: (-41.37656163408782, 28.870659314053988)\n",
            "Episode 80\n",
            "                    \tAverage Score: -87.09609907696907\n",
            "                    \tLast Score: -80.15555555555576\n",
            "                    \tEval Score: (-38.84168965312192, 27.166677684492807)\n",
            "Episode 85\n",
            "                    \tAverage Score: -86.35472807128741\n",
            "                    \tLast Score: -69.10794223826741\n",
            "                    \tEval Score: (-21.0696216981543, 45.693415545393705)\n",
            "Episode 90\n",
            "                    \tAverage Score: -85.1536744575719\n",
            "                    \tLast Score: -70.33082706766955\n",
            "                    \tEval Score: (-60.30607035559452, 8.090930403868104)\n",
            "Episode 95\n",
            "                    \tAverage Score: -83.80387160337968\n",
            "                    \tLast Score: -57.93883495145655\n",
            "                    \tEval Score: (-59.49500000000021, 3.0882240491547477)\n",
            "Episode 100\n",
            "                    \tAverage Score: -82.41023128185545\n",
            "                    \tLast Score: -53.97342657342684\n",
            "                    \tEval Score: (-57.2914617634882, 5.190345947905029)\n",
            "Episode 105\n",
            "                    \tAverage Score: -79.30849944913912\n",
            "                    \tLast Score: -54.5189189189193\n",
            "                    \tEval Score: (-55.198484012621876, 4.418572719534329)\n",
            "Episode 110\n",
            "                    \tAverage Score: -76.04121652100868\n",
            "                    \tLast Score: -61.71111111111141\n",
            "                    \tEval Score: (-47.56125270742545, 25.45378978888868)\n",
            "Episode 115\n",
            "                    \tAverage Score: -74.43064755929669\n",
            "                    \tLast Score: -139.96357615894127\n",
            "                    \tEval Score: (-109.89999999999998, 0.0)\n",
            "Episode 120\n",
            "                    \tAverage Score: -74.26737741686507\n",
            "                    \tLast Score: -131.14920634920665\n",
            "                    \tEval Score: (-109.89999999999998, 0.0)\n",
            "Episode 125\n",
            "                    \tAverage Score: -76.33485649050064\n",
            "                    \tLast Score: -129.332432432433\n",
            "                    \tEval Score: (-109.89999999999998, 0.0)\n",
            "Episode 130\n",
            "                    \tAverage Score: -78.54999423943418\n",
            "                    \tLast Score: -105.27878787878782\n",
            "                    \tEval Score: (-109.89999999999998, 0.0)\n",
            "Episode 135\n",
            "                    \tAverage Score: -80.37585697705327\n",
            "                    \tLast Score: -89.62456140350884\n",
            "                    \tEval Score: (-9.831784886182195, 43.96290193104406)\n",
            "Episode 140\n",
            "                    \tAverage Score: -81.7966017349737\n",
            "                    \tLast Score: -60.635051546392035\n",
            "                    \tEval Score: (-39.15531042517268, 35.118426361633894)\n",
            "Episode 145\n",
            "                    \tAverage Score: -81.27005948483534\n",
            "                    \tLast Score: -71.38929889298912\n",
            "                    \tEval Score: (-7.4150192488831035, 53.320367221959124)\n",
            "Episode 150\n",
            "                    \tAverage Score: -80.40355381236651\n",
            "                    \tLast Score: -76.21986754966913\n",
            "                    \tEval Score: (-56.332099729201516, 4.716224817216178)\n",
            "Episode 155\n",
            "                    \tAverage Score: -79.75132925051624\n",
            "                    \tLast Score: -51.33287671232901\n",
            "                    \tEval Score: (-55.95015442511423, 2.2239893100544776)\n",
            "Episode 160\n",
            "                    \tAverage Score: -78.77167041606131\n",
            "                    \tLast Score: -27.766101694914738\n",
            "                    \tEval Score: (-32.542228854155795, 33.01377491720087)\n",
            "Episode 165\n",
            "                    \tAverage Score: -77.56516098368668\n",
            "                    \tLast Score: -69.79731543624189\n",
            "                    \tEval Score: (-57.052728243833926, 3.366952154665132)\n",
            "Episode 170\n",
            "                    \tAverage Score: -75.19005180515889\n",
            "                    \tLast Score: -55.45755395683476\n",
            "                    \tEval Score: (-20.43632285437711, 44.139883411596365)\n",
            "Episode 175\n",
            "                    \tAverage Score: -72.50775829150315\n",
            "                    \tLast Score: -67.96807817589601\n",
            "                    \tEval Score: (-43.59153727052214, 25.939533596460297)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2da448fafba3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-30d9202c0384>\u001b[0m in \u001b[0;36mreinforce\u001b[0;34m(policy, optimizer, n_training_episodes, gamma, print_every)\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0;31m\\\u001b[0m\u001b[0mtAverage\u001b[0m \u001b[0mScore\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_deque\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0;31m\\\u001b[0m\u001b[0mtLast\u001b[0m \u001b[0mScore\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \\tEval Score: {evaluate_agent(eval_env,5,policy)}''')\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-3dc90fd46641>\u001b[0m in \u001b[0;36mevaluate_agent\u001b[0;34m(env, n_eval_episodes, policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# As in practical 4, we do not consider \"truncated\" (i.e., reaching the max number of steps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# as a termination condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"state_pixels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mWINDOW_W\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWINDOW_H\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_road\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         self.car.draw(\n\u001b[1;32m    619\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36m_render_road\u001b[0;34m(self, zoom, translation, angle)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 grass.append(\n\u001b[0m\u001b[1;32m    670\u001b[0m                     [\n\u001b[1;32m    671\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0mGRASS_DIM\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mGRASS_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRASS_DIM\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "reinforce(policy, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85P7ChOqN7c8",
        "outputId": "507d3997-1d9a-44cd-a59d-755b5373487d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "policy.device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "hTqv8LscQrgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = torch.load('model.pt', map_location=device)\n",
        "policy = policy.to(device)\n",
        "policy.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLb0dedAQtx4",
        "outputId": "cfbb48a5-c355-4cc0-dceb-90812bfc65e5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Policy(\n",
              "  (conv1): Conv2d(4, 32, kernel_size=(7, 7), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=5, bias=True)\n",
              "  (gs): Grayscale(num_output_channels=1)\n",
              "  (rs): Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def play_agent(env, policy):\n",
        "    total_reward = 0\n",
        "    state = env.reset()\n",
        "    for i in range(60):\n",
        "        state,_,_,_,_ = env.step(0)\n",
        "    step = 0\n",
        "    done = False\n",
        "    negative_reward_patience = MAX_PATIENCE\n",
        "    states = deque(maxlen=4)\n",
        "    for i in range(policy.n_frames):\n",
        "        states.append(state)\n",
        "    while not done:\n",
        "        action, _ = policy.act(states, exploration=False)\n",
        "        new_state, reward, done, _, _ = env.step(action)\n",
        "        states.append(new_state)\n",
        "        if reward >=0:\n",
        "            negative_reward_patience = MAX_PATIENCE\n",
        "        else:\n",
        "            negative_reward_patience -= 1\n",
        "            if negative_reward_patience == 0:\n",
        "                done = True\n",
        "        if done:\n",
        "            reward = -100\n",
        "        total_reward += reward\n",
        "        env.render()\n",
        "        if done:\n",
        "            break\n",
        "        state = new_state\n",
        "    print(\"Total Reward:\", total_reward)"
      ],
      "metadata": {
        "id": "cq28HU1UTEBo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_agent(eval_env, policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRWzLGZVTMiV",
        "outputId": "823268d1-5b75-413b-aab3-8341e8239700"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reward: -56.131506849315265\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}