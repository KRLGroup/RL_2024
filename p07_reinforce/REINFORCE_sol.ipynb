{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "VjX_2mvrN7c1",
    "outputId": "739f2177-ada7-4f2b-8166-0151790723ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_nqTKTwODqj",
    "outputId": "b7d99f31-d263-4fca-f4be-5757ca29b61e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /home/andrea/miniconda3/lib/python3.12/site-packages/pyglet-1.5.27-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /home/andrea/miniconda3/lib/python3.12/site-packages/gym_notices-0.0.8-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /home/andrea/miniconda3/lib/python3.12/site-packages/gym-0.26.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gymnasium in /home/andrea/miniconda3/lib/python3.12/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/andrea/miniconda3/lib/python3.12/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/andrea/miniconda3/lib/python3.12/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/andrea/miniconda3/lib/python3.12/site-packages (from gymnasium) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/andrea/miniconda3/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Aq405erfpGKv"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raB0pq_vuaak",
    "outputId": "58b2e7f3-6224-40ee-ced3-ef6bd1a42f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "x5Y4Y3eXugTc"
   },
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "\n",
    "# Create the envs\n",
    "env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "\n",
    "# Get the state space and action space\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NpSZGAuKulI6"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, n_actions, hidden_size, device=torch.device('cpu')):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "    def act(self, state, exploration=True):\n",
    "        # Get Action Probabilities\n",
    "        probs = self.forward(state).cpu()\n",
    "\n",
    "        # Return Action and LogProb\n",
    "        action = probs.argmax(-1)\n",
    "        log_prob = None\n",
    "        if exploration:\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample()\n",
    "            log_prob = m.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def to(self, device):\n",
    "        ret = super().to(device)\n",
    "        ret.device = device\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sYk1se-R3vmh"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, n_eval_episodes, policy):\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state, _ = env.reset() # state reset\n",
    "        done = trunc = False\n",
    "\n",
    "        # stats\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        while not done and not trunc:\n",
    "            # perform action\n",
    "            action, _ = policy.act(torch.tensor(state).to(device), exploration=False)\n",
    "\n",
    "            state, reward, done, trunc, _ = env.step(action)\n",
    "\n",
    "            # stats\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "        # stats\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "\n",
    "    # stats\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Uzb4bInRxMsx"
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes=50, gamma=0.99, print_every=10, target_eval_score=500):\n",
    "    # stats\n",
    "    scores_deque = deque(maxlen=100)\n",
    "\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = [] # stores log probs during episode\n",
    "        rewards = [] # stores rewards during episode\n",
    "\n",
    "        # init episode\n",
    "        state, _ = env.reset()\n",
    "        done = trunc = False\n",
    "\n",
    "        while not done and not trunc:\n",
    "            action, log_prob = policy.act(torch.tensor(state).to(device))\n",
    "\n",
    "            saved_log_probs.append(log_prob)\n",
    "\n",
    "            state, reward, done, trunc, _ = env.step(action)\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "        scores_deque.append(sum(rewards))\n",
    "\n",
    "        rewards = np.array(rewards)\n",
    "        discounts = np.power(gamma, np.arange(len(rewards)))\n",
    "\n",
    "        policy_loss = 0\n",
    "        for t in range(len(rewards)):\n",
    "            G = (discounts[:len(rewards)-t]*rewards[t:]).sum()\n",
    "            policy_loss += -(gamma**t)*G*saved_log_probs[t]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            eval_score = evaluate_agent(eval_env,5,policy)\n",
    "            print(f'''Episode {i_episode}\n",
    "                    \\tAverage Score: {np.mean(scores_deque)}\n",
    "                    \\tLast Score: {rewards.sum()}\n",
    "                    \\tEval Score: {eval_score[0]}''')\n",
    "            torch.save(policy, 'model.pt')\n",
    "            if eval_score[0] >= target_eval_score:\n",
    "                print(f\"Environment solved in {i_episode} episodes!\")\n",
    "                break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GwVv1ETxN7c6"
   },
   "outputs": [],
   "source": [
    "policy = Policy(env.observation_space.shape[0], n_actions, 32).to(device)\n",
    "policy = policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bQYqOdbiy0ez"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "esS1pRU6D9CS",
    "outputId": "022bf43b-7292-4757-9183-6cf61226f22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\n",
      "                    \tAverage Score: 22.2\n",
      "                    \tLast Score: 52.0\n",
      "                    \tEval Score: 9.0\n",
      "Episode 20\n",
      "                    \tAverage Score: 19.35\n",
      "                    \tLast Score: 14.0\n",
      "                    \tEval Score: 10.6\n",
      "Episode 30\n",
      "                    \tAverage Score: 21.5\n",
      "                    \tLast Score: 22.0\n",
      "                    \tEval Score: 14.0\n",
      "Episode 40\n",
      "                    \tAverage Score: 23.175\n",
      "                    \tLast Score: 43.0\n",
      "                    \tEval Score: 16.6\n",
      "Episode 50\n",
      "                    \tAverage Score: 23.26\n",
      "                    \tLast Score: 14.0\n",
      "                    \tEval Score: 19.0\n",
      "Episode 60\n",
      "                    \tAverage Score: 24.316666666666666\n",
      "                    \tLast Score: 39.0\n",
      "                    \tEval Score: 20.0\n",
      "Episode 70\n",
      "                    \tAverage Score: 24.242857142857144\n",
      "                    \tLast Score: 9.0\n",
      "                    \tEval Score: 24.6\n",
      "Episode 80\n",
      "                    \tAverage Score: 25.35\n",
      "                    \tLast Score: 19.0\n",
      "                    \tEval Score: 33.2\n",
      "Episode 90\n",
      "                    \tAverage Score: 24.81111111111111\n",
      "                    \tLast Score: 13.0\n",
      "                    \tEval Score: 44.8\n",
      "Episode 100\n",
      "                    \tAverage Score: 25.65\n",
      "                    \tLast Score: 41.0\n",
      "                    \tEval Score: 45.6\n",
      "Episode 110\n",
      "                    \tAverage Score: 26.96\n",
      "                    \tLast Score: 47.0\n",
      "                    \tEval Score: 45.6\n",
      "Episode 120\n",
      "                    \tAverage Score: 28.58\n",
      "                    \tLast Score: 25.0\n",
      "                    \tEval Score: 68.6\n",
      "Episode 130\n",
      "                    \tAverage Score: 29.28\n",
      "                    \tLast Score: 36.0\n",
      "                    \tEval Score: 67.0\n",
      "Episode 140\n",
      "                    \tAverage Score: 29.98\n",
      "                    \tLast Score: 48.0\n",
      "                    \tEval Score: 75.6\n",
      "Episode 150\n",
      "                    \tAverage Score: 31.16\n",
      "                    \tLast Score: 24.0\n",
      "                    \tEval Score: 116.6\n",
      "Episode 160\n",
      "                    \tAverage Score: 32.01\n",
      "                    \tLast Score: 50.0\n",
      "                    \tEval Score: 179.0\n",
      "Episode 170\n",
      "                    \tAverage Score: 33.01\n",
      "                    \tLast Score: 42.0\n",
      "                    \tEval Score: 247.0\n",
      "Episode 180\n",
      "                    \tAverage Score: 33.23\n",
      "                    \tLast Score: 18.0\n",
      "                    \tEval Score: 135.4\n",
      "Episode 190\n",
      "                    \tAverage Score: 35.08\n",
      "                    \tLast Score: 19.0\n",
      "                    \tEval Score: 102.2\n",
      "Episode 200\n",
      "                    \tAverage Score: 35.54\n",
      "                    \tLast Score: 72.0\n",
      "                    \tEval Score: 82.4\n",
      "Episode 210\n",
      "                    \tAverage Score: 37.32\n",
      "                    \tLast Score: 41.0\n",
      "                    \tEval Score: 123.2\n",
      "Episode 220\n",
      "                    \tAverage Score: 38.86\n",
      "                    \tLast Score: 79.0\n",
      "                    \tEval Score: 95.0\n",
      "Episode 230\n",
      "                    \tAverage Score: 40.78\n",
      "                    \tLast Score: 86.0\n",
      "                    \tEval Score: 141.2\n",
      "Episode 240\n",
      "                    \tAverage Score: 42.22\n",
      "                    \tLast Score: 27.0\n",
      "                    \tEval Score: 241.6\n",
      "Episode 250\n",
      "                    \tAverage Score: 43.93\n",
      "                    \tLast Score: 46.0\n",
      "                    \tEval Score: 253.6\n",
      "Episode 260\n",
      "                    \tAverage Score: 44.56\n",
      "                    \tLast Score: 41.0\n",
      "                    \tEval Score: 259.6\n",
      "Episode 270\n",
      "                    \tAverage Score: 46.56\n",
      "                    \tLast Score: 102.0\n",
      "                    \tEval Score: 166.2\n",
      "Episode 280\n",
      "                    \tAverage Score: 48.51\n",
      "                    \tLast Score: 45.0\n",
      "                    \tEval Score: 82.6\n",
      "Episode 290\n",
      "                    \tAverage Score: 49.11\n",
      "                    \tLast Score: 25.0\n",
      "                    \tEval Score: 192.0\n",
      "Episode 300\n",
      "                    \tAverage Score: 52.23\n",
      "                    \tLast Score: 38.0\n",
      "                    \tEval Score: 269.6\n",
      "Episode 310\n",
      "                    \tAverage Score: 52.77\n",
      "                    \tLast Score: 93.0\n",
      "                    \tEval Score: 206.6\n",
      "Episode 320\n",
      "                    \tAverage Score: 54.68\n",
      "                    \tLast Score: 46.0\n",
      "                    \tEval Score: 276.6\n",
      "Episode 330\n",
      "                    \tAverage Score: 55.63\n",
      "                    \tLast Score: 43.0\n",
      "                    \tEval Score: 161.0\n",
      "Episode 340\n",
      "                    \tAverage Score: 56.89\n",
      "                    \tLast Score: 81.0\n",
      "                    \tEval Score: 188.4\n",
      "Episode 350\n",
      "                    \tAverage Score: 57.29\n",
      "                    \tLast Score: 40.0\n",
      "                    \tEval Score: 270.6\n",
      "Episode 360\n",
      "                    \tAverage Score: 58.53\n",
      "                    \tLast Score: 40.0\n",
      "                    \tEval Score: 127.6\n",
      "Episode 370\n",
      "                    \tAverage Score: 61.75\n",
      "                    \tLast Score: 57.0\n",
      "                    \tEval Score: 208.4\n",
      "Episode 380\n",
      "                    \tAverage Score: 63.21\n",
      "                    \tLast Score: 125.0\n",
      "                    \tEval Score: 189.2\n",
      "Episode 390\n",
      "                    \tAverage Score: 66.72\n",
      "                    \tLast Score: 40.0\n",
      "                    \tEval Score: 170.6\n",
      "Episode 400\n",
      "                    \tAverage Score: 66.83\n",
      "                    \tLast Score: 44.0\n",
      "                    \tEval Score: 118.8\n",
      "Episode 410\n",
      "                    \tAverage Score: 68.53\n",
      "                    \tLast Score: 101.0\n",
      "                    \tEval Score: 101.2\n",
      "Episode 420\n",
      "                    \tAverage Score: 70.66\n",
      "                    \tLast Score: 21.0\n",
      "                    \tEval Score: 165.6\n",
      "Episode 430\n",
      "                    \tAverage Score: 71.39\n",
      "                    \tLast Score: 74.0\n",
      "                    \tEval Score: 299.2\n",
      "Episode 440\n",
      "                    \tAverage Score: 73.15\n",
      "                    \tLast Score: 51.0\n",
      "                    \tEval Score: 307.0\n",
      "Episode 450\n",
      "                    \tAverage Score: 75.79\n",
      "                    \tLast Score: 56.0\n",
      "                    \tEval Score: 194.6\n",
      "Episode 460\n",
      "                    \tAverage Score: 79.62\n",
      "                    \tLast Score: 140.0\n",
      "                    \tEval Score: 254.6\n",
      "Episode 470\n",
      "                    \tAverage Score: 81.21\n",
      "                    \tLast Score: 57.0\n",
      "                    \tEval Score: 343.8\n",
      "Episode 480\n",
      "                    \tAverage Score: 86.47\n",
      "                    \tLast Score: 264.0\n",
      "                    \tEval Score: 328.8\n",
      "Episode 490\n",
      "                    \tAverage Score: 90.29\n",
      "                    \tLast Score: 73.0\n",
      "                    \tEval Score: 439.2\n",
      "Episode 500\n",
      "                    \tAverage Score: 97.93\n",
      "                    \tLast Score: 123.0\n",
      "                    \tEval Score: 359.6\n",
      "Episode 510\n",
      "                    \tAverage Score: 102.04\n",
      "                    \tLast Score: 27.0\n",
      "                    \tEval Score: 462.0\n",
      "Episode 520\n",
      "                    \tAverage Score: 107.71\n",
      "                    \tLast Score: 82.0\n",
      "                    \tEval Score: 395.4\n",
      "Episode 530\n",
      "                    \tAverage Score: 113.95\n",
      "                    \tLast Score: 162.0\n",
      "                    \tEval Score: 453.4\n",
      "Episode 540\n",
      "                    \tAverage Score: 123.72\n",
      "                    \tLast Score: 93.0\n",
      "                    \tEval Score: 460.8\n",
      "Episode 550\n",
      "                    \tAverage Score: 130.27\n",
      "                    \tLast Score: 115.0\n",
      "                    \tEval Score: 477.2\n",
      "Episode 560\n",
      "                    \tAverage Score: 139.86\n",
      "                    \tLast Score: 215.0\n",
      "                    \tEval Score: 484.0\n",
      "Episode 570\n",
      "                    \tAverage Score: 151.63\n",
      "                    \tLast Score: 244.0\n",
      "                    \tEval Score: 500.0\n",
      "Environment solved in 570 episodes!\n"
     ]
    }
   ],
   "source": [
    "reinforce(policy, optimizer, n_training_episodes=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85P7ChOqN7c8",
    "outputId": "507d3997-1d9a-44cd-a59d-755b5373487d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqv8LscQrgn"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLb0dedAQtx4",
    "outputId": "cfbb48a5-c355-4cc0-dceb-90812bfc65e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38926/2062629924.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy = torch.load('model.pt', map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = torch.load('model.pt', map_location=device)\n",
    "policy = policy.to(device)\n",
    "policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_frames_as_gif(frames, path='./', filename='REINFORCE.gif'):\n",
    "\n",
    "    #Mess with this to change frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_agent(env, policy):\n",
    "    total_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    frames_gif=[]\n",
    "    step = 0\n",
    "    done = trunc = False\n",
    "    while not done and not trunc:\n",
    "        action, _ = policy.act(torch.tensor(state).to(device), exploration=False)\n",
    "        new_state, reward, done, trunc, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        frames_gif.append(env.render())\n",
    "        state = new_state\n",
    "    save_frames_as_gif(frames_gif)\n",
    "    print(\"Total Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 500.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "play_agent(eval_env, policy)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ta-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
